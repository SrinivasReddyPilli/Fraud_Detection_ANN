{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "RANDOM_SEED =7124\n",
    "LABELS = [\"Normal\", \"Fraud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 30)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the data and lookng at the structure of the data. This data has 30 attributes and 1 lakh records\n",
    "data = pd.read_csv(\"Fraud_data_amtstd.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.836500</td>\n",
       "      <td>-0.545419</td>\n",
       "      <td>-0.462979</td>\n",
       "      <td>0.537174</td>\n",
       "      <td>-0.426143</td>\n",
       "      <td>-0.100606</td>\n",
       "      <td>-0.584764</td>\n",
       "      <td>-0.103956</td>\n",
       "      <td>2.268429</td>\n",
       "      <td>-0.365185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085111</td>\n",
       "      <td>0.410736</td>\n",
       "      <td>0.137625</td>\n",
       "      <td>0.602906</td>\n",
       "      <td>-0.350260</td>\n",
       "      <td>0.464407</td>\n",
       "      <td>-0.070917</td>\n",
       "      <td>-0.030486</td>\n",
       "      <td>0.049882</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.289880</td>\n",
       "      <td>-2.576061</td>\n",
       "      <td>-0.092256</td>\n",
       "      <td>1.976405</td>\n",
       "      <td>2.810033</td>\n",
       "      <td>-2.669128</td>\n",
       "      <td>-0.981883</td>\n",
       "      <td>-0.470310</td>\n",
       "      <td>-0.025692</td>\n",
       "      <td>0.099528</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.473240</td>\n",
       "      <td>-0.307295</td>\n",
       "      <td>-2.789549</td>\n",
       "      <td>0.578976</td>\n",
       "      <td>-0.837979</td>\n",
       "      <td>0.372843</td>\n",
       "      <td>0.353451</td>\n",
       "      <td>-1.662202</td>\n",
       "      <td>-0.347171</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.131318</td>\n",
       "      <td>0.139818</td>\n",
       "      <td>0.586921</td>\n",
       "      <td>1.069291</td>\n",
       "      <td>-0.334908</td>\n",
       "      <td>-0.204938</td>\n",
       "      <td>-0.135526</td>\n",
       "      <td>0.043821</td>\n",
       "      <td>-0.121117</td>\n",
       "      <td>0.182139</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028126</td>\n",
       "      <td>-0.167062</td>\n",
       "      <td>-0.048054</td>\n",
       "      <td>-0.009912</td>\n",
       "      <td>0.417694</td>\n",
       "      <td>-0.479793</td>\n",
       "      <td>0.024360</td>\n",
       "      <td>0.023878</td>\n",
       "      <td>-0.208963</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.866956</td>\n",
       "      <td>1.373947</td>\n",
       "      <td>1.948343</td>\n",
       "      <td>2.686750</td>\n",
       "      <td>-0.366790</td>\n",
       "      <td>0.568632</td>\n",
       "      <td>-0.278349</td>\n",
       "      <td>0.739536</td>\n",
       "      <td>-1.655955</td>\n",
       "      <td>0.708396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022719</td>\n",
       "      <td>-0.070619</td>\n",
       "      <td>-0.080307</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.092167</td>\n",
       "      <td>0.159131</td>\n",
       "      <td>0.157940</td>\n",
       "      <td>-0.014370</td>\n",
       "      <td>-0.253595</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.842670</td>\n",
       "      <td>1.401843</td>\n",
       "      <td>0.927235</td>\n",
       "      <td>1.070402</td>\n",
       "      <td>0.843883</td>\n",
       "      <td>0.467333</td>\n",
       "      <td>0.366716</td>\n",
       "      <td>0.616739</td>\n",
       "      <td>-1.586963</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036573</td>\n",
       "      <td>-0.182581</td>\n",
       "      <td>-0.226834</td>\n",
       "      <td>-1.029794</td>\n",
       "      <td>-0.118762</td>\n",
       "      <td>-0.228960</td>\n",
       "      <td>-0.024250</td>\n",
       "      <td>0.046547</td>\n",
       "      <td>-0.346230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.178458</td>\n",
       "      <td>0.166055</td>\n",
       "      <td>-0.101567</td>\n",
       "      <td>0.369453</td>\n",
       "      <td>0.017198</td>\n",
       "      <td>-0.722891</td>\n",
       "      <td>0.396639</td>\n",
       "      <td>-0.187978</td>\n",
       "      <td>-0.483147</td>\n",
       "      <td>0.083094</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323048</td>\n",
       "      <td>-1.083814</td>\n",
       "      <td>0.049838</td>\n",
       "      <td>-0.002872</td>\n",
       "      <td>0.295810</td>\n",
       "      <td>0.135883</td>\n",
       "      <td>-0.074191</td>\n",
       "      <td>0.004364</td>\n",
       "      <td>-0.150527</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.869017</td>\n",
       "      <td>-0.202287</td>\n",
       "      <td>-0.218739</td>\n",
       "      <td>1.496434</td>\n",
       "      <td>-0.403332</td>\n",
       "      <td>-0.013593</td>\n",
       "      <td>-0.342586</td>\n",
       "      <td>0.129402</td>\n",
       "      <td>0.911017</td>\n",
       "      <td>0.149568</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.458963</td>\n",
       "      <td>-1.058509</td>\n",
       "      <td>0.439679</td>\n",
       "      <td>-0.066668</td>\n",
       "      <td>-0.376792</td>\n",
       "      <td>-1.125226</td>\n",
       "      <td>0.053537</td>\n",
       "      <td>-0.039380</td>\n",
       "      <td>-0.305050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.335053</td>\n",
       "      <td>0.331464</td>\n",
       "      <td>-2.057763</td>\n",
       "      <td>-0.346175</td>\n",
       "      <td>2.583234</td>\n",
       "      <td>2.854102</td>\n",
       "      <td>-0.187547</td>\n",
       "      <td>0.685154</td>\n",
       "      <td>-0.286614</td>\n",
       "      <td>-0.535903</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.191820</td>\n",
       "      <td>-0.650118</td>\n",
       "      <td>-0.114069</td>\n",
       "      <td>0.915936</td>\n",
       "      <td>0.730073</td>\n",
       "      <td>0.383879</td>\n",
       "      <td>-0.031902</td>\n",
       "      <td>0.029849</td>\n",
       "      <td>-0.347171</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.787763</td>\n",
       "      <td>-0.737892</td>\n",
       "      <td>-0.185794</td>\n",
       "      <td>0.362758</td>\n",
       "      <td>-0.550775</td>\n",
       "      <td>0.676564</td>\n",
       "      <td>-0.932369</td>\n",
       "      <td>0.390445</td>\n",
       "      <td>1.349983</td>\n",
       "      <td>-0.136095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.331025</td>\n",
       "      <td>1.223539</td>\n",
       "      <td>0.264791</td>\n",
       "      <td>-0.541170</td>\n",
       "      <td>-0.571339</td>\n",
       "      <td>0.812785</td>\n",
       "      <td>0.017984</td>\n",
       "      <td>-0.054691</td>\n",
       "      <td>-0.232691</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.055540</td>\n",
       "      <td>0.942471</td>\n",
       "      <td>0.986697</td>\n",
       "      <td>1.560551</td>\n",
       "      <td>-0.138755</td>\n",
       "      <td>-0.253645</td>\n",
       "      <td>0.622974</td>\n",
       "      <td>-0.321826</td>\n",
       "      <td>-0.215914</td>\n",
       "      <td>0.816454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153985</td>\n",
       "      <td>0.985031</td>\n",
       "      <td>0.204281</td>\n",
       "      <td>0.455561</td>\n",
       "      <td>-0.456576</td>\n",
       "      <td>-0.244140</td>\n",
       "      <td>-0.833487</td>\n",
       "      <td>-0.419773</td>\n",
       "      <td>-0.173549</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0  1.836500 -0.545419 -0.462979  0.537174 -0.426143 -0.100606 -0.584764   \n",
       "1 -4.289880 -2.576061 -0.092256  1.976405  2.810033 -2.669128 -0.981883   \n",
       "2  1.131318  0.139818  0.586921  1.069291 -0.334908 -0.204938 -0.135526   \n",
       "3 -0.866956  1.373947  1.948343  2.686750 -0.366790  0.568632 -0.278349   \n",
       "4 -0.842670  1.401843  0.927235  1.070402  0.843883  0.467333  0.366716   \n",
       "5  1.178458  0.166055 -0.101567  0.369453  0.017198 -0.722891  0.396639   \n",
       "6  1.869017 -0.202287 -0.218739  1.496434 -0.403332 -0.013593 -0.342586   \n",
       "7  1.335053  0.331464 -2.057763 -0.346175  2.583234  2.854102 -0.187547   \n",
       "8  1.787763 -0.737892 -0.185794  0.362758 -0.550775  0.676564 -0.932369   \n",
       "9 -1.055540  0.942471  0.986697  1.560551 -0.138755 -0.253645  0.622974   \n",
       "\n",
       "         V8        V9       V10  ...         V21       V22       V23  \\\n",
       "0 -0.103956  2.268429 -0.365185  ...    0.085111  0.410736  0.137625   \n",
       "1 -0.470310 -0.025692  0.099528  ...   -0.473240 -0.307295 -2.789549   \n",
       "2  0.043821 -0.121117  0.182139  ...   -0.028126 -0.167062 -0.048054   \n",
       "3  0.739536 -1.655955  0.708396  ...    0.022719 -0.070619 -0.080307   \n",
       "4  0.616739 -1.586963  0.000041  ...    0.036573 -0.182581 -0.226834   \n",
       "5 -0.187978 -0.483147  0.083094  ...   -0.323048 -1.083814  0.049838   \n",
       "6  0.129402  0.911017  0.149568  ...   -0.458963 -1.058509  0.439679   \n",
       "7  0.685154 -0.286614 -0.535903  ...   -0.191820 -0.650118 -0.114069   \n",
       "8  0.390445  1.349983 -0.136095  ...    0.331025  1.223539  0.264791   \n",
       "9 -0.321826 -0.215914  0.816454  ...    0.153985  0.985031  0.204281   \n",
       "\n",
       "        V24       V25       V26       V27       V28    Amount  Class  \n",
       "0  0.602906 -0.350260  0.464407 -0.070917 -0.030486  0.049882      0  \n",
       "1  0.578976 -0.837979  0.372843  0.353451 -1.662202 -0.347171      0  \n",
       "2 -0.009912  0.417694 -0.479793  0.024360  0.023878 -0.208963      0  \n",
       "3  0.000816  0.092167  0.159131  0.157940 -0.014370 -0.253595      0  \n",
       "4 -1.029794 -0.118762 -0.228960 -0.024250  0.046547 -0.346230      0  \n",
       "5 -0.002872  0.295810  0.135883 -0.074191  0.004364 -0.150527      0  \n",
       "6 -0.066668 -0.376792 -1.125226  0.053537 -0.039380 -0.305050      0  \n",
       "7  0.915936  0.730073  0.383879 -0.031902  0.029849 -0.347171      0  \n",
       "8 -0.541170 -0.571339  0.812785  0.017984 -0.054691 -0.232691      0  \n",
       "9  0.455561 -0.456576 -0.244140 -0.833487 -0.419773 -0.173549      0  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at a sample of records\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    99508\n",
      "1      492\n",
      "Name: Class, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLotting the frequencies of fraud and non-fraud transactions in the data\n",
    "count_classes = pd.value_counts(data['Class'], sort = True)\n",
    "print(count_classes)\n",
    "\n",
    "#Drawing a barplot\n",
    "count_classes.plot(kind = 'bar', rot=0)\n",
    "\n",
    "#Giving titles and labels to the plot\n",
    "plt.title(\"Transaction class distribution\")\n",
    "plt.xticks(range(2), LABELS)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Converting data to array\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 30)\n",
      "(20000, 30)\n"
     ]
    }
   ],
   "source": [
    "#Splitting the data into train and test and observing their dimensions\n",
    "X_train, X_test = train_test_split(data, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1.]), array([79591,   409], dtype=int64))\n",
      "(array([0., 1.]), array([19917,    83], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "#Obtaining the fraud and non-fraud records in train\n",
    "print(np.unique(X_train[:,29],return_counts=True))\n",
    "print(np.unique(X_test[:,29],return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79591, 29)\n"
     ]
    }
   ],
   "source": [
    "#Now consider only the non-fraud records for training\n",
    "X_train_NF = X_train[X_train[:,-1] == 0]\n",
    "X_train_NF = X_train_NF[:,:-1]\n",
    "print(X_train_NF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(409, 30)\n"
     ]
    }
   ],
   "source": [
    "#Separating out the fraud records from the train \n",
    "X_train_F = X_train[X_train[:,-1] == 1]\n",
    "print(X_train_F.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20818, 30)\n"
     ]
    }
   ],
   "source": [
    "#Adding/concatenating the fraud records from train data to the test\n",
    "X_test=np.concatenate((X_test,X_train_F),axis=0)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,X_eval = train_test_split(X_test, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16654, 30)\n",
      "(4164, 30)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(X_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating the independent and the class variable\n",
    "y_test = X_test[:,-1]\n",
    "X_test = X_test[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16654, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Expanding the dimensions of y for later concatenation\n",
    "y_test = np.expand_dims(y_test, axis=1)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_NF.shape[1]\n",
    "encoding_dim = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Sequential()\n",
    "\n",
    "autoencoder.add(Dropout(0.2, input_shape=(input_dim,)))\n",
    "autoencoder.add(Dense(encoding_dim, activation='relu'))\n",
    "autoencoder.add(Dense(input_dim, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 32\n",
    "\n",
    "autoencoder.compile(optimizer='adam', \n",
    "                    loss='mean_squared_error', \n",
    "                    metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 43us/step - loss: 0.6996 - mean_squared_error: 0.6996 - val_loss: 0.3801 - val_mean_squared_error: 0.3801\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 39us/step - loss: 0.4164 - mean_squared_error: 0.4164 - val_loss: 0.2655 - val_mean_squared_error: 0.2655\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 38us/step - loss: 0.3702 - mean_squared_error: 0.3702 - val_loss: 0.2411 - val_mean_squared_error: 0.2411\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 38us/step - loss: 0.3583 - mean_squared_error: 0.3583 - val_loss: 0.2298 - val_mean_squared_error: 0.2298\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 39us/step - loss: 0.3531 - mean_squared_error: 0.3531 - val_loss: 0.2292 - val_mean_squared_error: 0.2292\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3494 - mean_squared_error: 0.3494 - val_loss: 0.2303 - val_mean_squared_error: 0.2303\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3459 - mean_squared_error: 0.3459 - val_loss: 0.2341 - val_mean_squared_error: 0.2341\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3426 - mean_squared_error: 0.3426 - val_loss: 0.2255 - val_mean_squared_error: 0.2255\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3429 - mean_squared_error: 0.3429 - val_loss: 0.2264 - val_mean_squared_error: 0.2264\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3410 - mean_squared_error: 0.3410 - val_loss: 0.2236 - val_mean_squared_error: 0.2236\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3395 - mean_squared_error: 0.3395 - val_loss: 0.2287 - val_mean_squared_error: 0.2287\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3387 - mean_squared_error: 0.3387 - val_loss: 0.2244 - val_mean_squared_error: 0.2244\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3398 - mean_squared_error: 0.3398 - val_loss: 0.2230 - val_mean_squared_error: 0.2230\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3419 - mean_squared_error: 0.3419 - val_loss: 0.2230 - val_mean_squared_error: 0.2230\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3393 - mean_squared_error: 0.3393 - val_loss: 0.2266 - val_mean_squared_error: 0.2266\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3347 - mean_squared_error: 0.3347 - val_loss: 0.2258 - val_mean_squared_error: 0.2258\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3364 - mean_squared_error: 0.3364 - val_loss: 0.2235 - val_mean_squared_error: 0.2235\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3373 - mean_squared_error: 0.3373 - val_loss: 0.2275 - val_mean_squared_error: 0.2275\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3373 - mean_squared_error: 0.3373 - val_loss: 0.2217 - val_mean_squared_error: 0.2217\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3353 - mean_squared_error: 0.3353 - val_loss: 0.2224 - val_mean_squared_error: 0.2224\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 39us/step - loss: 0.3338 - mean_squared_error: 0.3338 - val_loss: 0.2229 - val_mean_squared_error: 0.2229\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 39us/step - loss: 0.3340 - mean_squared_error: 0.3340 - val_loss: 0.2221 - val_mean_squared_error: 0.2221\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 39us/step - loss: 0.3346 - mean_squared_error: 0.3346 - val_loss: 0.2242 - val_mean_squared_error: 0.2242\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 39us/step - loss: 0.3344 - mean_squared_error: 0.3344 - val_loss: 0.2226 - val_mean_squared_error: 0.2226\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3361 - mean_squared_error: 0.3361 - val_loss: 0.2224 - val_mean_squared_error: 0.2224\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 3s 45us/step - loss: 0.3362 - mean_squared_error: 0.3362 - val_loss: 0.2234 - val_mean_squared_error: 0.2234\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3341 - mean_squared_error: 0.3341 - val_loss: 0.2256 - val_mean_squared_error: 0.2256\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3332 - mean_squared_error: 0.3332 - val_loss: 0.2256 - val_mean_squared_error: 0.2256\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3330 - mean_squared_error: 0.3330 - val_loss: 0.2236 - val_mean_squared_error: 0.2236\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3318 - mean_squared_error: 0.3318 - val_loss: 0.2229 - val_mean_squared_error: 0.2229\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 44us/step - loss: 0.3304 - mean_squared_error: 0.3304 - val_loss: 0.2240 - val_mean_squared_error: 0.2240\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3340 - mean_squared_error: 0.3340 - val_loss: 0.2237 - val_mean_squared_error: 0.2237\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3293 - mean_squared_error: 0.3293 - val_loss: 0.2235 - val_mean_squared_error: 0.2235\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 42us/step - loss: 0.3287 - mean_squared_error: 0.3287 - val_loss: 0.2250 - val_mean_squared_error: 0.2250\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3288 - mean_squared_error: 0.3288 - val_loss: 0.2244 - val_mean_squared_error: 0.2244\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 44us/step - loss: 0.3314 - mean_squared_error: 0.3314 - val_loss: 0.2241 - val_mean_squared_error: 0.2241\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3297 - mean_squared_error: 0.3297 - val_loss: 0.2248 - val_mean_squared_error: 0.2248\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 39us/step - loss: 0.3274 - mean_squared_error: 0.3274 - val_loss: 0.2259 - val_mean_squared_error: 0.2259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 38us/step - loss: 0.3259 - mean_squared_error: 0.3259 - val_loss: 0.2275 - val_mean_squared_error: 0.2275\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 38us/step - loss: 0.3283 - mean_squared_error: 0.3283 - val_loss: 0.2259 - val_mean_squared_error: 0.2259\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 38us/step - loss: 0.3290 - mean_squared_error: 0.3290 - val_loss: 0.2243 - val_mean_squared_error: 0.2243\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 38us/step - loss: 0.3287 - mean_squared_error: 0.3287 - val_loss: 0.2272 - val_mean_squared_error: 0.2272\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3278 - mean_squared_error: 0.3278 - val_loss: 0.2251 - val_mean_squared_error: 0.2251\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3279 - mean_squared_error: 0.3279 - val_loss: 0.2247 - val_mean_squared_error: 0.2247\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3277 - mean_squared_error: 0.3277 - val_loss: 0.2251 - val_mean_squared_error: 0.2251\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3277 - mean_squared_error: 0.3277 - val_loss: 0.2254 - val_mean_squared_error: 0.2254\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3221 - mean_squared_error: 0.3221 - val_loss: 0.2252 - val_mean_squared_error: 0.2252\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3280 - mean_squared_error: 0.3280 - val_loss: 0.2243 - val_mean_squared_error: 0.2243\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3233 - mean_squared_error: 0.3233 - val_loss: 0.2234 - val_mean_squared_error: 0.2234\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3273 - mean_squared_error: 0.3273 - val_loss: 0.2251 - val_mean_squared_error: 0.2251\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3282 - mean_squared_error: 0.3282 - val_loss: 0.2249 - val_mean_squared_error: 0.2249\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 39us/step - loss: 0.3237 - mean_squared_error: 0.3237 - val_loss: 0.2231 - val_mean_squared_error: 0.2231\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3235 - mean_squared_error: 0.3235 - val_loss: 0.2242 - val_mean_squared_error: 0.2242\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3227 - mean_squared_error: 0.3227 - val_loss: 0.2242 - val_mean_squared_error: 0.2242\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3266 - mean_squared_error: 0.3266 - val_loss: 0.2239 - val_mean_squared_error: 0.2239\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 43us/step - loss: 0.3261 - mean_squared_error: 0.3261 - val_loss: 0.2234 - val_mean_squared_error: 0.2234\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 39us/step - loss: 0.3249 - mean_squared_error: 0.3249 - val_loss: 0.2242 - val_mean_squared_error: 0.2242\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 39us/step - loss: 0.3262 - mean_squared_error: 0.3262 - val_loss: 0.2234 - val_mean_squared_error: 0.2234\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3296 - mean_squared_error: 0.3296 - val_loss: 0.2259 - val_mean_squared_error: 0.2259\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 39us/step - loss: 0.3263 - mean_squared_error: 0.3263 - val_loss: 0.2250 - val_mean_squared_error: 0.2250\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 39us/step - loss: 0.3251 - mean_squared_error: 0.3251 - val_loss: 0.2230 - val_mean_squared_error: 0.2230\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3280 - mean_squared_error: 0.3280 - val_loss: 0.2233 - val_mean_squared_error: 0.2233\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3240 - mean_squared_error: 0.3240 - val_loss: 0.2234 - val_mean_squared_error: 0.2234\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 39us/step - loss: 0.3249 - mean_squared_error: 0.3249 - val_loss: 0.2239 - val_mean_squared_error: 0.2239\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3246 - mean_squared_error: 0.3246 - val_loss: 0.2230 - val_mean_squared_error: 0.2230\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3263 - mean_squared_error: 0.3263 - val_loss: 0.2234 - val_mean_squared_error: 0.2234\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3280 - mean_squared_error: 0.3280 - val_loss: 0.2239 - val_mean_squared_error: 0.2239\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 39us/step - loss: 0.3227 - mean_squared_error: 0.3227 - val_loss: 0.2241 - val_mean_squared_error: 0.2241\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3250 - mean_squared_error: 0.3250 - val_loss: 0.2233 - val_mean_squared_error: 0.2233\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3251 - mean_squared_error: 0.3251 - val_loss: 0.2238 - val_mean_squared_error: 0.2238\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 39us/step - loss: 0.3244 - mean_squared_error: 0.3244 - val_loss: 0.2258 - val_mean_squared_error: 0.2258\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3244 - mean_squared_error: 0.3244 - val_loss: 0.2222 - val_mean_squared_error: 0.2222\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3274 - mean_squared_error: 0.3274 - val_loss: 0.2250 - val_mean_squared_error: 0.2250\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 43us/step - loss: 0.3252 - mean_squared_error: 0.3252 - val_loss: 0.2258 - val_mean_squared_error: 0.2258\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 39us/step - loss: 0.3234 - mean_squared_error: 0.3234 - val_loss: 0.2245 - val_mean_squared_error: 0.2245\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3281 - mean_squared_error: 0.3281 - val_loss: 0.2269 - val_mean_squared_error: 0.2269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 39us/step - loss: 0.3257 - mean_squared_error: 0.3257 - val_loss: 0.2226 - val_mean_squared_error: 0.2226\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 38us/step - loss: 0.3272 - mean_squared_error: 0.3272 - val_loss: 0.2246 - val_mean_squared_error: 0.2246\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 38us/step - loss: 0.3222 - mean_squared_error: 0.3222 - val_loss: 0.2227 - val_mean_squared_error: 0.2227\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 38us/step - loss: 0.3270 - mean_squared_error: 0.3270 - val_loss: 0.2240 - val_mean_squared_error: 0.2240\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 38us/step - loss: 0.3281 - mean_squared_error: 0.3281 - val_loss: 0.2230 - val_mean_squared_error: 0.2230\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 38us/step - loss: 0.3258 - mean_squared_error: 0.3258 - val_loss: 0.2260 - val_mean_squared_error: 0.2260\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 38us/step - loss: 0.3235 - mean_squared_error: 0.3235 - val_loss: 0.2248 - val_mean_squared_error: 0.2248\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 38us/step - loss: 0.3255 - mean_squared_error: 0.3255 - val_loss: 0.2236 - val_mean_squared_error: 0.2236\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 38us/step - loss: 0.3240 - mean_squared_error: 0.3240 - val_loss: 0.2241 - val_mean_squared_error: 0.2241\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 38us/step - loss: 0.3233 - mean_squared_error: 0.3233 - val_loss: 0.2229 - val_mean_squared_error: 0.2229\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 39us/step - loss: 0.3233 - mean_squared_error: 0.3233 - val_loss: 0.2236 - val_mean_squared_error: 0.2236\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3272 - mean_squared_error: 0.3272 - val_loss: 0.2247 - val_mean_squared_error: 0.2247\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3256 - mean_squared_error: 0.3256 - val_loss: 0.2263 - val_mean_squared_error: 0.2263\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3265 - mean_squared_error: 0.3265 - val_loss: 0.2265 - val_mean_squared_error: 0.2265\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3237 - mean_squared_error: 0.3237 - val_loss: 0.2246 - val_mean_squared_error: 0.2246\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3242 - mean_squared_error: 0.3242 - val_loss: 0.2225 - val_mean_squared_error: 0.2225\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 3s 46us/step - loss: 0.3234 - mean_squared_error: 0.3234 - val_loss: 0.2263 - val_mean_squared_error: 0.2263\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 41us/step - loss: 0.3303 - mean_squared_error: 0.3303 - val_loss: 0.2254 - val_mean_squared_error: 0.2254\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3254 - mean_squared_error: 0.3254 - val_loss: 0.2234 - val_mean_squared_error: 0.2234\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3242 - mean_squared_error: 0.3242 - val_loss: 0.2234 - val_mean_squared_error: 0.2234\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 43us/step - loss: 0.3230 - mean_squared_error: 0.3230 - val_loss: 0.2233 - val_mean_squared_error: 0.2233\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3279 - mean_squared_error: 0.3279 - val_loss: 0.2250 - val_mean_squared_error: 0.2250\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 40us/step - loss: 0.3243 - mean_squared_error: 0.3243 - val_loss: 0.2250 - val_mean_squared_error: 0.2250\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s 44us/step - loss: 0.3233 - mean_squared_error: 0.3233 - val_loss: 0.2240 - val_mean_squared_error: 0.2240\n"
     ]
    }
   ],
   "source": [
    "hist = []\n",
    "for _ in range(100):\n",
    "    hist.append(autoencoder.fit(X_train_NF, X_train_NF,\n",
    "                    epochs=1,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_split=0.3,\n",
    "                    verbose=1).history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'val_loss': [0.38005120787915897],\n",
       "  'val_mean_squared_error': [0.38005120787915897],\n",
       "  'loss': [0.6995537196713345],\n",
       "  'mean_squared_error': [0.6995537196713345]},\n",
       " {'val_loss': [0.26545556668301046],\n",
       "  'val_mean_squared_error': [0.26545556668301046],\n",
       "  'loss': [0.4163782178693383],\n",
       "  'mean_squared_error': [0.4163782178693383]},\n",
       " {'val_loss': [0.2411345892052014],\n",
       "  'val_mean_squared_error': [0.2411345892052014],\n",
       "  'loss': [0.3702467727968287],\n",
       "  'mean_squared_error': [0.3702467727968287]},\n",
       " {'val_loss': [0.22976043123312856],\n",
       "  'val_mean_squared_error': [0.22976043123312856],\n",
       "  'loss': [0.35831103631477523],\n",
       "  'mean_squared_error': [0.35831103631477523]},\n",
       " {'val_loss': [0.22916333335424868],\n",
       "  'val_mean_squared_error': [0.22916333335424868],\n",
       "  'loss': [0.35310092635584206],\n",
       "  'mean_squared_error': [0.35310092635584206]}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making predictions on the train data\n",
    "predictions=autoencoder.predict(X_train_NF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.64127493e+00,  8.64178300e-01,  1.67897320e+00,\n",
       "         8.42323303e-01, -8.36604238e-01,  5.69799602e-01,\n",
       "        -5.76715708e-01,  1.15235829e+00,  4.97222126e-01,\n",
       "        -7.53913224e-01, -1.43701839e+00,  3.60802770e-01,\n",
       "        -9.79655087e-01, -1.69507980e-01, -1.80152750e+00,\n",
       "        -4.43888962e-01,  4.60821897e-01, -3.46859545e-02,\n",
       "         1.03266783e-01,  1.59416050e-02, -1.05432838e-01,\n",
       "         9.83073562e-02, -1.65940747e-02, -5.82004450e-02,\n",
       "        -1.28182536e-02,  5.91138452e-02, -1.24624707e-02,\n",
       "         3.44233885e-02, -2.07527161e-01],\n",
       "       [ 1.34040797e+00, -3.50826621e-01,  1.29697084e-01,\n",
       "        -4.45682406e-01, -7.14094222e-01, -1.46277916e+00,\n",
       "        -1.30317450e-01, -3.39976668e-01, -1.06451082e+00,\n",
       "         7.09093571e-01,  1.55913830e-01,  5.11288643e-04,\n",
       "         4.06116664e-01,  1.64511085e-01,  1.88157082e-01,\n",
       "        -5.57780862e-02, -1.22675836e-01, -9.70042646e-02,\n",
       "        -6.66358992e-02, -3.92480046e-02, -1.01521313e-02,\n",
       "         3.51510122e-02,  8.55222344e-04, -3.82963307e-02,\n",
       "        -1.31579274e-02, -1.56537239e-02, -1.31192207e-02,\n",
       "         7.48829916e-03, -1.72951221e-01]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16654, 30)\n",
      "(15931, 30)\n",
      "(723, 30)\n"
     ]
    }
   ],
   "source": [
    "##We want to separate out fraud records and non-fraud records for later use\n",
    "f = np.hstack((X_test,y_test))\n",
    "print(f.shape)\n",
    "\n",
    "test_nf=f[f[:,29]==0]\n",
    "print(test_nf.shape)\n",
    "\n",
    "test_f=f[f[:,29]==1]\n",
    "print(test_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15931/15931 [==============================] - 0s 16us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.225147445524871, 0.225147445524871]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the errors from the non fraud data separately \n",
    "autoencoder.evaluate(test_nf[:,:29],test_nf[:,:29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "723/723 [==============================] - 0s 68us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[19.16120733891615, 19.16120733891615]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the errors from the fraud data separately\n",
    "autoencoder.evaluate(test_f[:,:29],test_f[:,:29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining predictions for non fraud records\n",
    "predictions_nf=autoencoder.predict(test_nf[:,:29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining predictions for fraud records\n",
    "predictions_f=autoencoder.predict(test_f[:,:29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22514744415513402"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Identifying the error computation method by autoencoder(Mean Squared Error). The computation is as follows \n",
    "np.mean(np.square(np.abs(test_nf[:,:29]-predictions_nf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing errors on the non-fraud data\n",
    "errors_nf = np.mean(np.square(np.abs(test_nf[:,:29]-predictions_nf)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03547175, 0.04206122, 0.41240355, 0.11981173, 0.06523133])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_nf[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing errors on the fraud data\n",
    "errors_f = np.mean(np.square(np.abs(test_f[:,:29]-predictions_f)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.5369563 , 63.54879695,  2.89292737,  0.41545727,  0.15957598])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_f[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01183803474417335\n",
      "111.87228290795944\n",
      "0.12085370560609035\n",
      "0.030985393238905787\n",
      "137.08643728987414\n",
      "7.965452229253221\n"
     ]
    }
   ],
   "source": [
    "#Computing the distribution of errors in both non-fraud and fraud data\n",
    "print(np.min(errors_nf))\n",
    "print(np.max(errors_nf))\n",
    "print(np.median(errors_nf))\n",
    "\n",
    "print(np.min(errors_f))\n",
    "print(np.max(errors_f))\n",
    "print(np.median(errors_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x17da05910b8>,\n",
       "  <matplotlib.lines.Line2D at 0x17da0591550>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x17da0591978>,\n",
       "  <matplotlib.lines.Line2D at 0x17da0591da0>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x17da0583f28>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x17da0598208>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x17da0598630>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLotting the error box plots \n",
    "\n",
    "plt.subplot(1, 2,1)\n",
    "plt.boxplot(errors_f)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(errors_nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "360\n",
      "41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7965"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Experimentation to fix a threshold for classification of a transaction into fraud or non-fraud\n",
    "print(sum(errors_nf>np.median(errors_f)))\n",
    "print(sum(errors_f<np.median(errors_f)))\n",
    "print(sum(errors_f<np.median(errors_nf)))\n",
    "sum(errors_nf>np.median(errors_nf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15931,)\n",
      "(723,)\n"
     ]
    }
   ],
   "source": [
    "print(errors_nf.shape)\n",
    "print(errors_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15931, 29)\n",
      "(723, 29)\n"
     ]
    }
   ],
   "source": [
    "print(predictions_nf.shape)\n",
    "print(predictions_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = autoencoder.predict(X_test[:,:29])\n",
    "test_recon  = (((test_pred-X_test)**2).mean(-1))\n",
    "\n",
    "train_pred = autoencoder.predict(X_train_NF[:,:29])\n",
    "mean_recon = (((train_pred - X_train_NF)**2).mean(-1).mean())\n",
    "\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,confusion_matrix\n",
    "\n",
    "scores_f1 = []\n",
    "thres = []\n",
    "\n",
    "th = 0\n",
    "for i in range(100):\n",
    "    th+=0.1\n",
    "    fraud = (test_recon>mean_recon+th)\n",
    "    scores_f1.append(f1_score(y_test,fraud))\n",
    "    thres.append(th+mean_recon)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16654, 29)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.825767052103589\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[15867,    64],\n",
       "       [  162,   561]], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thres, scores_f1)\n",
    "\n",
    "print(thres[np.array(scores_f1).argmax()])\n",
    "\n",
    "fraud = (test_recon>thres[np.array(scores_f1).argmax()])\n",
    "\n",
    "confusion_matrix(y_test, fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Predicting on Valdation \n",
    "\n",
    "predictions_eval=autoencoder.predict(X_eval[:,:29])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_eval=np.square(np.subtract(predictions_eval,X_eval[:,:29]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_eval=(((errors_eval-X_eval[:,:29])**2).mean(-1))>2.925861360803925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3678  308]\n",
      " [  29  149]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_true=X_eval[:,29],y_pred=fraud_eval))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
